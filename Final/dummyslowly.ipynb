{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0f55f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Generating 1000 system monitoring records with gradual patterns and ~5% anomalies...\n",
      "⚠️ No anomalies were generated in this run\n",
      "✅ Unsupervised dataset generated successfully!\n",
      "📁 Dataset saved as: system_monitoring_unsupervised.csv\n",
      "📊 Dataset shape: (2000, 11)\n",
      "🎯 Contains 0 anomalous records (details logged separately)\n",
      "\n",
      "📋 Sample of generated data (first 5 records):\n",
      "fw_load_avg_1_min fw_load_avg_5_min fw_load_avg_15_min  fw_cpu_used  mem_used  root_used  log_used  fw_total_alloc  total_rx_packets  total_tx_packets              created_at\n",
      "             0,13              0,22               0,18            0   7909922   10093214  10308128       645970186          19001258           1820113 2024-11-06 08:01:10.657\n",
      "             0,20              0,29               0,11            1   7908706   10093269  10308047       645873105          19003739           1820008 2024-11-06 08:03:37.727\n",
      "             0,00              0,28               0,21            0   7910752   10093227  10308305       646125338          19002663           1820022 2024-11-06 08:05:21.266\n",
      "             0,08              0,10               0,22            0   7906569   10093285  10308644       646044252          19002419           1820188 2024-11-06 08:06:49.318\n",
      "             0,10              0,01               0,18            0   7908655   10093147  10308169       645914832          19004798           1820171 2024-11-06 08:09:04.071\n",
      "\n",
      "📋 Sample of generated data (last 5 records - showing progression):\n",
      "fw_load_avg_1_min fw_load_avg_5_min fw_load_avg_15_min  fw_cpu_used  mem_used  root_used  log_used  fw_total_alloc  total_rx_packets  total_tx_packets              created_at\n",
      "             0,16              0,28               0,21            1   7927275   10093294  10316444       647701935          19150723           1824151 2024-11-09 02:30:39.991\n",
      "             0,22              0,28               0,24            0   7927219   10093249  10316417       647746752          19152207           1824077 2024-11-09 02:33:15.460\n",
      "             0,24              0,32               0,33            0   7925568   10093209  10315554       648346149          19152943           1824066 2024-11-09 02:36:22.953\n",
      "             0,35              0,18               0,26            0   7927725   10093214  10316392       647816327          19154150           1824112 2024-11-09 02:38:52.779\n",
      "             0,28              0,16               0,21            0   7928182   10093278  10315543       648337002          19153940           1824150 2024-11-09 02:39:52.147\n",
      "\n",
      "📈 Data statistics:\n",
      "Date range: 2024-11-06 08:01:10.657 to 2024-11-09 02:39:52.147\n",
      "Total records: 2000\n",
      "Load avg 1min range: 0.00 to 0.55\n",
      "CPU usage range: 0 to 2\n",
      "Memory usage range: 7,902,570 to 7,933,479\n",
      "RX packets range: 19,000,554 to 19,154,150\n",
      "TX packets range: 1,820,008 to 1,824,169\n",
      "\n",
      "📈 Showing gradual increase pattern:\n",
      "First record memory: 7,909,922\n",
      "Last record memory: 7,928,182\n",
      "Memory increase: 18,260\n",
      "First record RX packets: 19,001,258\n",
      "Last record RX packets: 19,153,940\n",
      "RX packets increase: 152,682\n",
      "\n",
      "💡 Dataset characteristics:\n",
      "✓ Purely unsupervised - no labels or anomaly indicators in main dataset\n",
      "✓ Gradual increase in memory usage over time\n",
      "✓ Gradual increase in network packets over time\n",
      "✓ Gradual increase in log usage over time\n",
      "✓ Load averages stay low with small variations\n",
      "✓ CPU usage mostly 0, occasionally 1-2\n",
      "✓ Realistic timestamp progression\n",
      "✓ Contains hidden anomalies (~5%) for detection algorithms\n",
      "✓ Anomalies timeline logged separately in anomalies_list.csv\n",
      "📝 Both files use semicolon (;) as separator\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "import random\n",
    "\n",
    "# Tanpa set seed:\n",
    "rand_array = np.random.rand(3)\n",
    "rand_int = random.randint(0, 100)\n",
    "\n",
    "\n",
    "def generate_system_monitoring_data(n_samples=2000, anomaly_ratio=0.00):\n",
    "    \"\"\"\n",
    "    Generate synthetic system monitoring data with gradual increasing patterns and anomalies\n",
    "    Also track anomalies for separate logging\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base timestamp - mulai dari waktu yang lebih awal untuk menunjukkan progression\n",
    "    start_time = datetime(2024, 11, 6, 8, 0, 0)\n",
    "    \n",
    "    data = []\n",
    "    anomaly_logs = []  # List untuk menyimpan informasi anomali\n",
    "    anomaly_indices = set(np.random.choice(n_samples, int(n_samples * anomaly_ratio), replace=False))\n",
    "    \n",
    "    # Base values yang akan naik secara gradual\n",
    "    base_mem = 7910000\n",
    "    base_root = 10093200\n",
    "    base_log = 10308000\n",
    "    base_fw_alloc = 646000000\n",
    "    base_rx = 19000000\n",
    "    base_tx = 1820000\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Time progression - interval yang lebih natural\n",
    "        time_offset = timedelta(\n",
    "            seconds=random.randint(30, 180),  # 30 detik sampai 3 menit\n",
    "            microseconds=random.randint(0, 999999)\n",
    "        )\n",
    "        current_time = start_time + timedelta(minutes=i * 2) + time_offset  # Setiap 2 menit rata-rata\n",
    "        \n",
    "        is_anomaly = i in anomaly_indices\n",
    "        \n",
    "        if is_anomaly:\n",
    "            # Generate anomalous data dan catat anomalinya\n",
    "            row, anomaly_info = generate_anomaly_row(current_time, i, n_samples, base_mem, base_root, base_log, base_fw_alloc, base_rx, base_tx)\n",
    "            \n",
    "            # Tambahkan informasi anomali ke log\n",
    "            anomaly_log_entry = {\n",
    "                'record_index': i,\n",
    "                'timestamp': current_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3],\n",
    "                'anomaly_type': anomaly_info['type'],\n",
    "                'description': anomaly_info['description'],\n",
    "                'affected_metrics': ', '.join(anomaly_info['affected_metrics']),\n",
    "                'severity': anomaly_info['severity']\n",
    "            }\n",
    "            anomaly_logs.append(anomaly_log_entry)\n",
    "        else:\n",
    "            # Generate normal data\n",
    "            row = generate_normal_row(current_time, i, n_samples, base_mem, base_root, base_log, base_fw_alloc, base_rx, base_tx)\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    # Sort by timestamp untuk memastikan urutan waktu yang benar\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values('created_at').reset_index(drop=True)\n",
    "    \n",
    "    # Create anomalies DataFrame\n",
    "    anomalies_df = pd.DataFrame(anomaly_logs)\n",
    "    if not anomalies_df.empty:\n",
    "        anomalies_df = anomalies_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    return df, anomalies_df, anomaly_indices\n",
    "\n",
    "def generate_normal_row(timestamp, index, total_samples, base_mem, base_root, base_log, base_fw_alloc, base_rx, base_tx):\n",
    "    \"\"\"Generate normal system metrics dengan gradual increase pattern\"\"\"\n",
    "    \n",
    "    # Progress factor (0 to 1) - menunjukkan seberapa jauh dalam timeline\n",
    "    progress = index / total_samples\n",
    "    \n",
    "    # Load averages - tetap rendah dengan variasi kecil\n",
    "    fw_load_avg_1_min = round(max(0, np.random.normal(0.15 + progress * 0.1, 0.1)), 2)\n",
    "    fw_load_avg_5_min = round(max(0, np.random.normal(0.18 + progress * 0.08, 0.08)), 2)\n",
    "    fw_load_avg_15_min = round(max(0, np.random.normal(0.20 + progress * 0.05, 0.05)), 2)\n",
    "    \n",
    "    # CPU usage - mostly 0, occasionally 1-2\n",
    "    cpu_base = 0 if random.random() < 0.7 else random.choice([1, 1, 2])\n",
    "    fw_cpu_used = cpu_base\n",
    "    \n",
    "    # Memory usage - gradual increase dengan noise\n",
    "    mem_increase = progress * 15000  # Naik sekitar 15KB selama periode\n",
    "    mem_used = int(base_mem + mem_increase + np.random.normal(0, 3000))\n",
    "    \n",
    "    # Root filesystem - hampir konstan dengan sedikit variasi\n",
    "    root_used = int(base_root + np.random.normal(0, 50))\n",
    "    \n",
    "    # Log usage - gradual increase (log files bertambah)\n",
    "    log_increase = progress * 8000  # Log naik sekitar 8KB\n",
    "    log_used = int(base_log + log_increase + np.random.normal(0, 500))\n",
    "    \n",
    "    # Firewall total allocation - gradual increase dengan variasi\n",
    "    fw_alloc_increase = progress * 2000000  # Naik sekitar 2MB\n",
    "    fw_total_alloc = int(base_fw_alloc + fw_alloc_increase + np.random.normal(0, 200000))\n",
    "    \n",
    "    # Network packets - gradual increase (traffic bertambah)\n",
    "    rx_increase = progress * 150000  # RX packets naik\n",
    "    tx_increase = progress * 4000    # TX packets naik\n",
    "    \n",
    "    total_rx_packets = int(base_rx + rx_increase + np.random.randint(0, 5000))\n",
    "    total_tx_packets = int(base_tx + tx_increase + np.random.randint(0, 200))\n",
    "    \n",
    "    return {\n",
    "        'fw_load_avg_1_min': f\"{fw_load_avg_1_min:.2f}\".replace('.', ','),\n",
    "        'fw_load_avg_5_min': f\"{fw_load_avg_5_min:.2f}\".replace('.', ','),\n",
    "        'fw_load_avg_15_min': f\"{fw_load_avg_15_min:.2f}\".replace('.', ','),\n",
    "        'fw_cpu_used': fw_cpu_used,\n",
    "        'mem_used': mem_used,\n",
    "        'root_used': root_used,\n",
    "        'log_used': log_used,\n",
    "        'fw_total_alloc': fw_total_alloc,\n",
    "        'total_rx_packets': total_rx_packets,\n",
    "        'total_tx_packets': total_tx_packets,\n",
    "        'created_at': timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "    }\n",
    "\n",
    "def generate_anomaly_row(timestamp, index, total_samples, base_mem, base_root, base_log, base_fw_alloc, base_rx, base_tx):\n",
    "    \"\"\"Generate anomalous system metrics and return anomaly information\"\"\"\n",
    "    \n",
    "    progress = index / total_samples\n",
    "    \n",
    "    anomaly_type = random.choice([\n",
    "        'high_load', 'high_cpu', 'memory_spike', 'disk_spike', \n",
    "        'network_spike', 'system_stress', 'resource_leak'\n",
    "    ])\n",
    "    \n",
    "    # Initialize anomaly info\n",
    "    anomaly_info = {\n",
    "        'type': anomaly_type,\n",
    "        'description': '',\n",
    "        'affected_metrics': [],\n",
    "        'severity': 'medium'\n",
    "    }\n",
    "    \n",
    "    if anomaly_type == 'high_load':\n",
    "        # Abnormally high load averages\n",
    "        fw_load_avg_1_min = round(np.random.uniform(2.0, 8.0), 2)\n",
    "        fw_load_avg_5_min = round(np.random.uniform(1.5, 4.0), 2)\n",
    "        fw_load_avg_15_min = round(np.random.uniform(1.0, 2.5), 2)\n",
    "        fw_cpu_used = random.randint(50, 95)\n",
    "        \n",
    "        anomaly_info['description'] = f'High system load detected: 1min={fw_load_avg_1_min}, CPU={fw_cpu_used}%'\n",
    "        anomaly_info['affected_metrics'] = ['fw_load_avg_1_min', 'fw_load_avg_5_min', 'fw_load_avg_15_min', 'fw_cpu_used']\n",
    "        anomaly_info['severity'] = 'high' if fw_load_avg_1_min > 5.0 else 'medium'\n",
    "        \n",
    "        # Base values dengan gradual increase\n",
    "        mem_used = int(base_mem + progress * 15000 + np.random.normal(0, 3000))\n",
    "        root_used = int(base_root + np.random.normal(0, 50))\n",
    "        log_used = int(base_log + progress * 8000 + np.random.normal(0, 500))\n",
    "        fw_total_alloc = int(base_fw_alloc + progress * 2000000 + np.random.normal(0, 200000))\n",
    "        total_rx_packets = int(base_rx + progress * 150000 + np.random.randint(0, 5000))\n",
    "        total_tx_packets = int(base_tx + progress * 4000 + np.random.randint(0, 200))\n",
    "        \n",
    "    elif anomaly_type == 'high_cpu':\n",
    "        # Normal load but very high CPU\n",
    "        fw_load_avg_1_min = round(max(0, np.random.normal(0.15 + progress * 0.1, 0.1)), 2)\n",
    "        fw_load_avg_5_min = round(max(0, np.random.normal(0.18 + progress * 0.08, 0.08)), 2)\n",
    "        fw_load_avg_15_min = round(max(0, np.random.normal(0.20 + progress * 0.05, 0.05)), 2)\n",
    "        fw_cpu_used = random.randint(80, 100)\n",
    "        \n",
    "        anomaly_info['description'] = f'High CPU usage detected: {fw_cpu_used}% with normal load average'\n",
    "        anomaly_info['affected_metrics'] = ['fw_cpu_used']\n",
    "        anomaly_info['severity'] = 'high' if fw_cpu_used > 90 else 'medium'\n",
    "        \n",
    "        mem_used = int(base_mem + progress * 15000 + np.random.normal(0, 3000))\n",
    "        root_used = int(base_root + np.random.normal(0, 50))\n",
    "        log_used = int(base_log + progress * 8000 + np.random.normal(0, 500))\n",
    "        fw_total_alloc = int(base_fw_alloc + progress * 2000000 + np.random.normal(0, 200000))\n",
    "        total_rx_packets = int(base_rx + progress * 150000 + np.random.randint(0, 5000))\n",
    "        total_tx_packets = int(base_tx + progress * 4000 + np.random.randint(0, 200))\n",
    "        \n",
    "    elif anomaly_type == 'memory_spike':\n",
    "        # Memory usage spike\n",
    "        fw_load_avg_1_min = round(max(0, np.random.normal(0.4, 0.2)), 2)\n",
    "        fw_load_avg_5_min = round(max(0, np.random.normal(0.35, 0.15)), 2)\n",
    "        fw_load_avg_15_min = round(max(0, np.random.normal(0.25, 0.1)), 2)\n",
    "        fw_cpu_used = random.randint(5, 25)\n",
    "        \n",
    "        # Memory spike - 2-3x normal\n",
    "        mem_used = int(np.random.uniform(15000000, 25000000))\n",
    "        \n",
    "        anomaly_info['description'] = f'Memory spike detected: {mem_used/1000000:.1f}MB (2-3x normal)'\n",
    "        anomaly_info['affected_metrics'] = ['mem_used', 'fw_load_avg_1_min']\n",
    "        anomaly_info['severity'] = 'high' if mem_used > 20000000 else 'medium'\n",
    "        \n",
    "        root_used = int(base_root + np.random.normal(0, 50))\n",
    "        log_used = int(base_log + progress * 8000 + np.random.normal(0, 500))\n",
    "        fw_total_alloc = int(base_fw_alloc + progress * 2000000 + np.random.normal(0, 200000))\n",
    "        total_rx_packets = int(base_rx + progress * 150000 + np.random.randint(0, 5000))\n",
    "        total_tx_packets = int(base_tx + progress * 4000 + np.random.randint(0, 200))\n",
    "        \n",
    "    elif anomaly_type == 'disk_spike':\n",
    "        # Disk usage anomaly\n",
    "        fw_load_avg_1_min = round(max(0, np.random.normal(0.8, 0.3)), 2)\n",
    "        fw_load_avg_5_min = round(max(0, np.random.normal(0.6, 0.2)), 2)\n",
    "        fw_load_avg_15_min = round(max(0, np.random.normal(0.4, 0.15)), 2)\n",
    "        fw_cpu_used = random.randint(15, 40)\n",
    "        \n",
    "        mem_used = int(base_mem + progress * 15000 + np.random.normal(0, 3000))\n",
    "        root_used = int(np.random.uniform(15000000, 20000000))  # Disk full\n",
    "        log_used = int(np.random.uniform(20000000, 30000000))   # Log explosion\n",
    "        \n",
    "        anomaly_info['description'] = f'Disk usage spike: Root={root_used/1000000:.1f}MB, Log={log_used/1000000:.1f}MB'\n",
    "        anomaly_info['affected_metrics'] = ['root_used', 'log_used', 'fw_load_avg_1_min']\n",
    "        anomaly_info['severity'] = 'critical' if root_used > 18000000 else 'high'\n",
    "        \n",
    "        fw_total_alloc = int(base_fw_alloc + progress * 2000000 + np.random.normal(0, 200000))\n",
    "        total_rx_packets = int(base_rx + progress * 150000 + np.random.randint(0, 5000))\n",
    "        total_tx_packets = int(base_tx + progress * 4000 + np.random.randint(0, 200))\n",
    "        \n",
    "    elif anomaly_type == 'network_spike':\n",
    "        # Network traffic anomaly\n",
    "        fw_load_avg_1_min = round(max(0, np.random.normal(0.6, 0.2)), 2)\n",
    "        fw_load_avg_5_min = round(max(0, np.random.normal(0.5, 0.15)), 2)\n",
    "        fw_load_avg_15_min = round(max(0, np.random.normal(0.3, 0.1)), 2)\n",
    "        fw_cpu_used = random.randint(10, 30)\n",
    "        \n",
    "        mem_used = int(base_mem + progress * 15000 + np.random.normal(0, 10000))\n",
    "        root_used = int(base_root + np.random.normal(0, 50))\n",
    "        log_used = int(base_log + progress * 8000 + np.random.normal(0, 500))\n",
    "        fw_total_alloc = int(np.random.uniform(800000000, 1200000000))  # High allocation\n",
    "        total_rx_packets = random.randint(25000000, 40000000)  # Traffic spike\n",
    "        total_tx_packets = random.randint(2500000, 4000000)\n",
    "        \n",
    "        anomaly_info['description'] = f'Network traffic spike: RX={total_rx_packets/1000000:.1f}M, TX={total_tx_packets/1000000:.1f}M packets'\n",
    "        anomaly_info['affected_metrics'] = ['total_rx_packets', 'total_tx_packets', 'fw_total_alloc']\n",
    "        anomaly_info['severity'] = 'high' if total_rx_packets > 35000000 else 'medium'\n",
    "        \n",
    "    elif anomaly_type == 'system_stress':\n",
    "        # Overall system stress\n",
    "        fw_load_avg_1_min = round(np.random.uniform(3.0, 6.0), 2)\n",
    "        fw_load_avg_5_min = round(np.random.uniform(2.0, 4.0), 2)\n",
    "        fw_load_avg_15_min = round(np.random.uniform(1.5, 3.0), 2)\n",
    "        fw_cpu_used = random.randint(70, 95)\n",
    "        \n",
    "        mem_used = int(np.random.uniform(12000000, 18000000))\n",
    "        root_used = int(np.random.uniform(12000000, 15000000))\n",
    "        log_used = int(np.random.uniform(15000000, 25000000))\n",
    "        fw_total_alloc = int(np.random.uniform(750000000, 950000000))\n",
    "        total_rx_packets = random.randint(22000000, 35000000)\n",
    "        total_tx_packets = random.randint(2200000, 3500000)\n",
    "        \n",
    "        anomaly_info['description'] = f'System stress: High load ({fw_load_avg_1_min}), CPU ({fw_cpu_used}%), Memory ({mem_used/1000000:.1f}MB)'\n",
    "        anomaly_info['affected_metrics'] = ['fw_load_avg_1_min', 'fw_cpu_used', 'mem_used', 'root_used', 'log_used']\n",
    "        anomaly_info['severity'] = 'critical'\n",
    "        \n",
    "    else:  # resource_leak\n",
    "        # Gradual resource leak pattern\n",
    "        fw_load_avg_1_min = round(np.random.uniform(1.0, 2.5), 2)\n",
    "        fw_load_avg_5_min = round(np.random.uniform(1.2, 2.8), 2)\n",
    "        fw_load_avg_15_min = round(np.random.uniform(1.5, 3.2), 2)\n",
    "        fw_cpu_used = random.randint(25, 60)\n",
    "        \n",
    "        # Resource leak - lebih tinggi dari normal tapi tidak extreme\n",
    "        mem_used = int(base_mem + progress * 15000 + np.random.uniform(3000000, 6000000))\n",
    "        root_used = int(base_root + np.random.normal(0, 50))\n",
    "        log_used = int(base_log + progress * 8000 + np.random.normal(0, 500))\n",
    "        fw_total_alloc = int(base_fw_alloc + progress * 2000000 + np.random.uniform(30000000, 100000000))\n",
    "        total_rx_packets = int(base_rx + progress * 150000 + np.random.randint(0, 5000))\n",
    "        total_tx_packets = int(base_tx + progress * 4000 + np.random.randint(0, 200))\n",
    "        \n",
    "        anomaly_info['description'] = f'Resource leak detected: Memory +{(mem_used-base_mem)/1000000:.1f}MB, FW alloc +{(fw_total_alloc-base_fw_alloc)/1000000:.1f}MB'\n",
    "        anomaly_info['affected_metrics'] = ['mem_used', 'fw_total_alloc', 'fw_load_avg_15_min']\n",
    "        anomaly_info['severity'] = 'medium'\n",
    "    \n",
    "    row_data = {\n",
    "        'fw_load_avg_1_min': f\"{fw_load_avg_1_min:.2f}\".replace('.', ','),\n",
    "        'fw_load_avg_5_min': f\"{fw_load_avg_5_min:.2f}\".replace('.', ','),\n",
    "        'fw_load_avg_15_min': f\"{fw_load_avg_15_min:.2f}\".replace('.', ','),\n",
    "        'fw_cpu_used': fw_cpu_used,\n",
    "        'mem_used': mem_used,\n",
    "        'root_used': root_used,\n",
    "        'log_used': log_used,\n",
    "        'fw_total_alloc': fw_total_alloc,\n",
    "        'total_rx_packets': total_rx_packets,\n",
    "        'total_tx_packets': total_tx_packets,\n",
    "        'created_at': timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "    }\n",
    "    \n",
    "    return row_data, anomaly_info\n",
    "\n",
    "# Generate the dataset\n",
    "print(\"🚀 Generating 1000 system monitoring records with gradual patterns and ~5% anomalies...\")\n",
    "df, anomalies_df, anomaly_indices = generate_system_monitoring_data()\n",
    "\n",
    "# Save complete dataset to CSV with semicolon separator\n",
    "output_file = 'system_monitoring_unsupervised.csv'\n",
    "df.to_csv(output_file, sep=';', index=False)\n",
    "\n",
    "# Save anomalies log to separate CSV\n",
    "anomalies_file = 'anomalies_list.csv'\n",
    "if not anomalies_df.empty:\n",
    "    anomalies_df.to_csv(anomalies_file, sep=';', index=False)\n",
    "    print(f\"🎯 Anomalies log saved as: {anomalies_file}\")\n",
    "else:\n",
    "    print(\"⚠️ No anomalies were generated in this run\")\n",
    "\n",
    "print(f\"✅ Unsupervised dataset generated successfully!\")\n",
    "print(f\"📁 Dataset saved as: {output_file}\")\n",
    "print(f\"📊 Dataset shape: {df.shape}\")\n",
    "print(f\"🎯 Contains {len(anomalies_df)} anomalous records (details logged separately)\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n📋 Sample of generated data (first 5 records):\")\n",
    "print(df.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\n📋 Sample of generated data (last 5 records - showing progression):\")\n",
    "print(df.tail(5).to_string(index=False))\n",
    "\n",
    "# Display anomalies summary if any exist\n",
    "if not anomalies_df.empty:\n",
    "    print(f\"\\n🚨 Anomalies Summary ({len(anomalies_df)} total):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Group by anomaly type\n",
    "    anomaly_counts = anomalies_df['anomaly_type'].value_counts()\n",
    "    for anomaly_type, count in anomaly_counts.items():\n",
    "        print(f\"  {anomaly_type}: {count} occurrences\")\n",
    "    \n",
    "    # Group by severity\n",
    "    severity_counts = anomalies_df['severity'].value_counts()\n",
    "    print(f\"\\nSeverity Distribution:\")\n",
    "    for severity, count in severity_counts.items():\n",
    "        print(f\"  {severity}: {count} occurrences\")\n",
    "    \n",
    "    print(f\"\\n📋 Sample anomalies (first 3):\")\n",
    "    print(anomalies_df[['timestamp', 'anomaly_type', 'severity', 'description']].head(3).to_string(index=False))\n",
    "\n",
    "print(\"\\n📈 Data statistics:\")\n",
    "print(f\"Date range: {df['created_at'].min()} to {df['created_at'].max()}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "\n",
    "# Convert comma decimal to dot for numeric analysis\n",
    "df_numeric = df.copy()\n",
    "for col in ['fw_load_avg_1_min', 'fw_load_avg_5_min', 'fw_load_avg_15_min']:\n",
    "    df_numeric[col] = df_numeric[col].str.replace(',', '.').astype(float)\n",
    "\n",
    "print(f\"Load avg 1min range: {df_numeric['fw_load_avg_1_min'].min():.2f} to {df_numeric['fw_load_avg_1_min'].max():.2f}\")\n",
    "print(f\"CPU usage range: {df['fw_cpu_used'].min()} to {df['fw_cpu_used'].max()}\")\n",
    "print(f\"Memory usage range: {df['mem_used'].min():,} to {df['mem_used'].max():,}\")\n",
    "print(f\"RX packets range: {df['total_rx_packets'].min():,} to {df['total_rx_packets'].max():,}\")\n",
    "print(f\"TX packets range: {df['total_tx_packets'].min():,} to {df['total_tx_packets'].max():,}\")\n",
    "\n",
    "print(\"\\n📈 Showing gradual increase pattern:\")\n",
    "print(f\"First record memory: {df.iloc[0]['mem_used']:,}\")\n",
    "print(f\"Last record memory: {df.iloc[-1]['mem_used']:,}\")\n",
    "print(f\"Memory increase: {df.iloc[-1]['mem_used'] - df.iloc[0]['mem_used']:,}\")\n",
    "\n",
    "print(f\"First record RX packets: {df.iloc[0]['total_rx_packets']:,}\")\n",
    "print(f\"Last record RX packets: {df.iloc[-1]['total_rx_packets']:,}\")\n",
    "print(f\"RX packets increase: {df.iloc[-1]['total_rx_packets'] - df.iloc[0]['total_rx_packets']:,}\")\n",
    "\n",
    "print(\"\\n💡 Dataset characteristics:\")\n",
    "print(\"✓ Purely unsupervised - no labels or anomaly indicators in main dataset\")\n",
    "print(\"✓ Gradual increase in memory usage over time\")\n",
    "print(\"✓ Gradual increase in network packets over time\") \n",
    "print(\"✓ Gradual increase in log usage over time\")\n",
    "print(\"✓ Load averages stay low with small variations\")\n",
    "print(\"✓ CPU usage mostly 0, occasionally 1-2\")\n",
    "print(\"✓ Realistic timestamp progression\")\n",
    "print(\"✓ Contains hidden anomalies (~5%) for detection algorithms\")\n",
    "print(\"✓ Anomalies timeline logged separately in anomalies_list.csv\")\n",
    "print(\"📝 Both files use semicolon (;) as separator\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
